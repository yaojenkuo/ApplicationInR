directory = lapply(temp, read.delim)
pollutantmean <- function(directory, pollutant, id = 1:332) {
## 'directory' is a character vector of length 1 indicating
## the location of the CSV files
temp = list.files(pattern="*.csv")
directory = lapply(temp, read.delim)
## 'pollutant' is a character vector of length 1 indicating
## the name of the pollutant for which we will calculate the
## mean; either "sulfate" or "nitrate".
## 'id' is an integer vector indicating the monitor ID numbers
## to be used
## Return the mean of the pollutant across all monitors list
## in the 'id' vector (ignoring NA values)
}
pollutantmean(directory)
pollutantmean <- function(directory, pollutant, id = 1:332) {
## 'directory' is a character vector of length 1 indicating
## the location of the CSV files
## 'pollutant' is a character vector of length 1 indicating
## the name of the pollutant for which we will calculate the
## mean; either "sulfate" or "nitrate".
## 'id' is an integer vector indicating the monitor ID numbers
## to be used
## Return the mean of the pollutant across all monitors list
## in the 'id' vector (ignoring NA values)
path = directory
fileList = list.files(path)
#extract the file names and store as numeric for comparison
file.names = as.numeric(sub("\\.csv$","",fileList))
#select files to be imported based on the user input or default
selected.files = fileList[match(id,file.names)]
#import data
Data = lapply(file.path(path,selected.files),read.csv)
#convert into data frame
Data = do.call(rbind.data.frame,Data)
#calculate mean
mean(Data[,pollutant],na.rm=TRUE)
}
pollutantmean(specdata, "niterate", 1:10)
pollutantmean(specdata, "niterate", 1:10)
find.package("devtools")
install.package("devtools")
install.packages("devtools")
library(devtools)
find_rtools()
find.package("devtools")
#1000 simulated averages of 40 exponentials
expodist<-rep(NA,1000)
for (i in 1:1000){expodist[i] <- mean(rexp(40,0.2))}
hist(expodist,probability=T, main="Distribution of averages of samples drawn from exponential distribution", ylim=c(0, 0.6))
#Simulated density curve
lines(density(expodist))
#Normal distribution density curve
xfit <- seq(min(expodist), max(expodist), length=100)
yfit <- dnorm(xfit, mean=1/0.2, sd=(1/0.2/sqrt(40)))
lines(xfit, yfit, pch=22, col="red")
#Theoretical mean
abline(v=1/0.2,col="blue")
#Add legend
legend('topright', c("simulation", "theoretical"), lty=c(1,1), col=c("black", "red"))
#Sample Mean versus Theoretical Mean
mean(expodist)
1/0.2
#Sample Variance versus Theoretical Variance
var(expodist)
(1/0.2)^2/40
0.0936/(0.0936+0.088)
pnorm(24,mean=21, sd=5)
qnorm(0.9,mean=21,sd=5)
qnorm(0.1,mean=21,sd=5)
(2.33*300/40)^2
(30.69-32)/(4.31/6)
?qnorm
pnorm(1.82,mean=0,sd=1)
pnorm(-1.82,mean=0,sd=1)
30.69+c(-1,1)*1.645*4.31/6
9.4/sqrt(507)
(1.96*300/25)^2
pnorm((134-130)/(17/sqrt(35)),mean=0,sd=1)
pnorm((134-130)/(17/sqrt(35)),mean=0,sd=1,lower.tail=TRUE)
pnorm((134-130)/(17/sqrt(35)),mean=0,sd=1,lower.tail=FALSE)
415+c(-1,1)*220/sqrt(100)
415+c(-1,1)*1.96*220/sqrt(100)
q()
pop_mean <- 10
sam_mean <- 9.51
sd <- 4.65
n <- 40
z <- (pop_mean-sam_mean)/(sd/sqrt(n))
pnorm(z)
z <- (sam_mean-pop_mean)/(sd/sqrt(n))
pnorm(z)
0.91*0.02+0.09*0.9
0.09*0.9/0.0992
(9/2)^2
(1.28*18/4)^2
q()
0.0499-0.0364
0.5*0.5/(0.01/1.96)^2
q()
104*(15/104)
104*(89/104)
sqrt(0.36*0.64/50)
26*7/50
24*7/50
150*0.08
150*0.05
q()
76
76+0.35*72+0.43*30
qt(0.025,df=394)
q()
sum(dbinom(4:20,20,0.1))
dbinom(2,5,0.2)
dbinom(1,5,0.2)
dbinom(2,10,0.1)
dbinom(2,10,0.2)
q()
sqrt(0.11*0.89/100)
1.96*0.03+0.11
1-0.88^4
195528-1627
1627/828
193901/2
(1627/828)/(193901/2)
1627/2
193901/828
813.5*234.18
813.5/234.18
pf(3.47,828,2,lower.tail=FALSE)
SE <- sqrt(p(1-p)/3226)
p <- 0.2
SE <- sqrt(p(1-p)/3226)
SE <- sqrt(p*(1-p)/3226)
p <- 0.24
SE
SE <- sqrt(p*(1-p)/3226)
10/0.64
10/0.36
440*331/1293
46*112/625
0.05*0.93/(0.05*0.93+0.95*0.03)
1-((3819.99/15079.02)*(251/243))
q()
1.54/10000
0.08/10000
(0.08/10000)*100
q()
library(datasets)
data(mtcars)
tapply(mtcars$mpg, mtcars$cyl, mean)
lapply(mtcars, mean)
apply(mtcars, 2, mean)
tapply(mtcars$mpg, mtcars$cyl, mean)
split(mtcars, mtcars$cyl)
sapply(mtcars, cyl, mean)
with(mtcars, tapply(mpg, cyl, mean))
q()
3999+880*2
quit()
q()
install.packages("AppliedPredictiveModeling")
install.packages("caret")
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
indexTrain <- segmentationOriginal$Case == "Train"
train <- segmentationOriginal[indexTrain, ]
test <- segmentationOriginal[-indexTrain, ]
set.seed(125)
cartModel <- train(Class ~ ., data=train, method="rpart")
cartModel$finalModel
plot(cartModel$finalModel, uniform=T)
text(cartModel$finalModel, cex=0.8)
```
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
treeModel <- train(Area ~ ., data=olive, method="rpart")
newdata = as.data.frame(t(colMeans(olive)))
predict(treeModel, newdata)
library(ElemStatLearn)
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
modelRandomforest <- randomForest(y ~ ., data = vowel.train)
varImp(modelRandomforest)
library(caret)
modelRandomforest <- randomForest(y ~ ., data = vowel.train)
install.packages("randomForest")
library(randomForest)
modelRandomforest <- randomForest(y ~ ., data = vowel.train)
varImp(modelRandomforest)
varImportance <- varImp(modelRandomforest)
order(varImportance)
View(varImportance)
order(varImportance$Overall)
order(varImportance$row.names)
modelRandomforest <- train(y ~ ., data = vowel.train, method="rf")
varImp(modelRandomforest)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
modelRF <- train(y ~ ., data=vowel.train, method="rf")
modelGBM <- train(y ~ ., data=vowel.train, method="gbm")
predictRF <- predict(modelRF, vowel.test)
predGBM <- predict(modelGBM, vowel.test)
confusionMatrix(predictRF, vowel.test$y)
confusionMatrix(predGBM, vowel.test$y)
pred <- data.frame(predictRF, predGBM, y=vowel.test$y, agree=predictRF == predGBM)
View(pred)
set.seed(62433)
modelRF <- train(diagnosis ~ ., data=training, method="rf")
modelGBM <- train(diagnosis ~ ., data=training, method="gbm")
modelLDA <- train(diagnosis ~ ., data=training, method="lda")
predictRF <- predict(modelRF, testing)
predictGBM <- predict(modelGBM, testing)
predictLDA <- predict(modelLDA, testing)
stackedPredict <- data.frame(predictRF, predictGBM, predictLDA, diagnosis=testing$diagnosis)
modelStacked <- train(diagnosis ~., data=stackedPredict, method="rf")
predictStacked <- predict(modelStacked, testing)
confusionMatrix(predictRF, testing$diagnosis)
confusionMatrix(predictGBM, testing$diagnosis)
confusionMatrix(predictLDA, testing$diagnosis)
confusionMatrix(predictStacked, testing$diagnosis)
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
modelRF <- train(diagnosis ~ ., data=training, method="rf")
modelGBM <- train(diagnosis ~ ., data=training, method="gbm")
modelLDA <- train(diagnosis ~ ., data=training, method="lda")
predictRF <- predict(modelRF, testing)
predictGBM <- predict(modelGBM, testing)
predictLDA <- predict(modelLDA, testing)
stackedPredict <- data.frame(predictRF, predictGBM, predictLDA, diagnosis=testing$diagnosis)
modelStacked <- train(diagnosis ~., data=stackedPredict, method="rf")
predictStacked <- predict(modelStacked, testing)
confusionMatrix(predictRF, testing$diagnosis)
confusionMatrix(predictGBM, testing$diagnosis)
confusionMatrix(predictLDA, testing$diagnosis)
confusionMatrix(predictStacked, testing$diagnosis)
install.packages("lubridate")
library(lubridate)  # For year() function below
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
dat = read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
modelBats <- bats(tstrain)
predictBats <- forecast(modelBats, level=95, h=dim(testing)[1])
install.packages("forecast")
modelBats <- bats(tstrain)
library(forecast)
modelBats <- bats(tstrain)
dim(testing)
dim(testing)[1]
nrow(testing)
predictBats <- forecast(modelBats, level=95, h=nrow(testing))
predictBats
combination <- data.frame(testing, predictBats)
View(combination)
combination$withinFlag <- combination$Lo.95<combination$visitsTumblr<combination$Hi.95
combination$withinFlag <- (combination$Lo.95<combination$visitsTumblr)&(combination$visitsTumblr<combination$Hi.95)
counts <- table(combination$withinFlag)
counts
prop.table(counts)
prop.table(counts)[2]
install.packages("e1071")
install.packages("e1071")
set.seed(325)
library(e1071)
modelSVM <- svm(CompressiveStrength ~., data=training)
library(e1071)
install.packages("e1071")
install.packages("e1071")
library(e1071)
modelSVM <- svm(CompressiveStrength ~., data=training)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
library(caret)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
library(e1071)
modelSVM <- svm(CompressiveStrength ~., data=training)
predictSVM <- predict(modelSVM, testing)
accuracySVM <- accuracy(predictSVM, testing$CompressiveStrength)
library(ElemStatLearn)
library(gbm)
accuracySVM <- accuracy(predictSVM, testing$CompressiveStrength)
library(forecast)
accuracySVM <- accuracy(predictSVM, testing$CompressiveStrength)
accuracySVM
set.seed(233)
modelLasso <- train(CompressiveStrength ~ ., data=training, method="lasso")
library(elasticnet)
modelLasso <- train(CompressiveStrength ~ ., data=training, method="lasso")
quit()
library(VIM)
str(sleep)
summary(sleep)
aggr(sleep, combined=T, numbers=T, prop=F, main='Missing Pattern of Sleep', ylab='')
indexInt <- sapply(sleep, is.integer)
sleepIntVar <- sleep[, indexInt]
sleepNumVar <- sleep[, !indexInt]
autoMfrow <- function(n){
if (n%2==0){
par(mfrow=c((n)/2,2))
}else{
par(mfrow=c((n+1)/2,2))
}
}
autoMfrow <- function(n) {
if (n%2==0){
par(mfrow=c((n)/2,2))
} else{
par(mfrow=c((n+1)/2,2))
}
}
autoMfrow <- function(numCnt) {
if (numCnt%2==0){
par(mfrow=c((numCnt)/2,2))
} else{
par(mfrow=c((numCnt+1)/2,2))
}
}
autoMfrow <- function(x){
if (x%%2==0){
par(mfrow=c((x)/2,2))
} else{
par(mfrow=c((x+1)/2,2))
}
}
numCnt <- ncol(sleepNumVar)
autoMfrow(numCnt)
quit()
library(VIM)
matrixplot(sleep)
quit()
library(rgeos)
library(maptools)
library(ggplot2)
setwd("C:/Users/ASUS/Documents/ApplicationInR/countyIndicatorTW")
twDist <- readRDS("TWN_adm2.rds")
twDist <- fortify(twDist, region = "NAME_3")
twDist <- fortify(twDist, region = "NAME_2")
head(twDist)
table(twDist$id)
twDist$id[twDist$id=='Taichung'] <- 'Taichung City'
twDist$id[twDist$id=='Tainan'] <- 'Tainan City'
twDist$id[twDist$id=='Kaohsiung'] <- 'Kaohsiung City'
table(twDist$id)
library(plyr)
twIndex <- read.csv("countyStat.csv")
twDist$id <- toupper(twDist$id)  #change ids to uppercase
twIndex$id <- toupper(twIndex$id)  #change ids to uppercase
twIndexAvg <- ddply(twIndex, .(id), summarize, oldPercentAvg = mean(oldPercent))
View(twIndexAvg)
twIndex <- read.csv("countyStat.csv")
twIndex$id <- toupper(twIndex$id)  #change ids to uppercase
twIndexAvg <- ddply(twIndex, .(id), summarize, oldPercentAvg = mean(oldPercent))
View(twIndexAvg)
View(twIndex)
twIndexAvg <- ddply(twIndex, .(id), summarize, oldPercentAvg = mean(oldPercent), socialIncreaseAvg=mean(socialIncrease), unemploymentAvg=mean(unemployment))
View(twIndexAvg)
ggplot() + geom_map(data=twIndex, aes(map_id = id, fill = oldPercent), map = twDist) + expand_limits(x = twDist$long, y = twDist$lat) + scale_fill_gradient2(low = muted("red"),  mid = "white", midpoint = 10, high = muted("blue"), limits = c(5, 20))
library(scales)
ggplot() + geom_map(data=twIndex, aes(map_id = id, fill = oldPercent), map = twDist) + expand_limits(x = twDist$long, y = twDist$lat) + scale_fill_gradient2(low = muted("red"),  mid = "white", midpoint = 10, high = muted("blue"), limits = c(5, 20))
distcenters <- ddply(twDist, .(id), summarize, clat = mean(lat), clong = mean(long))
distCenters <- ddply(twDist, .(id), summarize, clat = mean(lat), clong = mean(long))
+ geom_text(data = distanceCenters, aes(x = longCenter, y = latCenter, label = id, size = 0.2))
distanceCenter <- ddply(twDist, .(id), summarize, latCenter = mean(lat), longCenter = mean(long))
+ geom_text(data = distanceCenters, aes(x = longCenter, y = latCenter, label = id, size = 0.2))
+ geom_text(data = distanceCenter, aes(x = longCenter, y = latCenter, label = id, size = 0.2))
View(distanceCenter)
rm(distCenters)
rm(distcenters)
ggplot()
+ geom_map(data=twIndex, aes(map_id = id, fill = oldPercent), map = twDist)
+ expand_limits(x = twDist$long, y = twDist$lat)
+ scale_fill_gradient2(low = muted("red"),  mid = "white", midpoint = 10, high = muted("blue"), limits = c(5, 20))
+ geom_text(data = distanceCenter, aes(x = longCenter, y = latCenter, label = id, size = 0.2))
View(distanceCenter)
ggplot() + geom_map(data=twIndex, aes(map_id = id, fill = oldPercent), map = twDist) + expand_limits(x = twDist$long, y = twDist$lat) + scale_fill_gradient2(low = muted("red"),  mid = "white", midpoint = 10, high = muted("blue"), limits = c(5, 20)) + geom_text(data = distanceCenter, aes(x = longCenter, y = latCenter, label = id, size = 0.2))
ggplot() + geom_map(data=twIndex, aes(map_id = id, fill = oldPercent), map = twDist) + expand_limits(x = twDist$long, y = twDist$lat) + scale_fill_gradient2(low = muted("red"),  mid = "white", midpoint = 14, high = muted("blue"), limits = c(8, 20)) + geom_text(data = distanceCenter, aes(x = longCenter, y = latCenter, label = id, size = 0.1))
ggplot() + geom_map(data=twIndex, aes(map_id = id, fill = oldPercent), map = twDist) + expand_limits(x = twDist$long, y = twDist$lat) + scale_fill_gradient2(low = muted("darkred"),  mid = "white", midpoint = 14, high = muted("darkblue"), limits = c(8, 20)) + geom_text(data = distanceCenter, aes(x = longCenter, y = latCenter, label = id, size = 0.1))
ggplot() + geom_map(data=twIndex, aes(map_id = id, fill = oldPercent), map = twDist) + expand_limits(x = twDist$long, y = twDist$lat) + scale_fill_gradient2(low = muted("darkred"),  high = muted("darkblue"), limits = c(8, 20)) + geom_text(data = distanceCenter, aes(x = longCenter, y = latCenter, label = id, size = 0.1))
ggplot() + geom_map(data=twIndex, aes(map_id = id, fill = oldPercent), map = twDist) + expand_limits(x = twDist$long, y = twDist$lat) + scale_fill_gradient2(low = muted("darkgreen"),  mid = "white", midpoint = 14, high = muted("darkblue"), limits = c(8, 20)) + geom_text(data = distanceCenter, aes(x = longCenter, y = latCenter, label = id, size = 0.1))
ggplot() + geom_map(data=twIndex, aes(map_id = id, fill = oldPercent), map = twDist) + expand_limits(x = twDist$long, y = twDist$lat) + scale_fill_gradient2(low = muted("darkgreen"),  mid = "white", midpoint = 14, high = muted("red"), limits = c(8, 20)) + geom_text(data = distanceCenter, aes(x = longCenter, y = latCenter, label = id, size = 0.1))
twDist <- getData(‘GADM’, country=’TW’, level=2)
twDist <- getData('GADM', country=’TW’, level=2)
twDist <- getData('GADM', country='TW', level=2)
library(raster)
twDist <- getData('GADM', country='TW', level=2)
twDist <- fortify(twDist, region = "NAME_2")
packageNames <- c("plyr", "ggplot2","rgeos", "maptools", "scales", "raster")
install.packages(packageNames)
install.packages(packageNames)
lapply(packageNames, library, character.only=TRUE)
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
x*w
mean(x*w)
sum(x*w)
setwd("C:/Users/ASUS/Documents/ApplicationInR/countyIndicatorTW")
twIndex <- read.csv("data/countyStat.csv")
mean(twIndex$oldPercent)
min(twIndex$oldPercent)
max(twIndex$oldPercent)
twDist2 <- getData('GADM', country='TW', level=2)
plot(twDist2)
png(filename = "choropleth1.png", width = 480, height = 308, units = "px")
dev.off()
setwd("C:/Users/ASUS/Documents/ApplicationInR/countyIndicatorTW")
twIndex <- read.csv("data/countyStat.csv")
twDist2 <- fortify(twDist2, region = "NAME_2")#fortify function helps us transform a SpatialPolygonDataFrame into a data frame, which is easier to manipulate.
twDist2$id[twDist2$id=='Taichung'] <- 'Taichung City'
twDist2$id[twDist2$id=='Tainan'] <- 'Tainan City'
twDist2$id[twDist2$id=='Kaohsiung'] <- 'Kaohsiung City'
twIndexAvg <- ddply(twIndex, .(id), summarize, oldPercentAvg = mean(oldPercent), socialIncreaseAvg=mean(socialIncrease), unemploymentAvg=mean(unemployment))#We just roughly average the indicators of 2 counties if they are merged in data from GADM.
ggplot() + geom_map(data=twIndex, aes(map_id = id, fill = oldPercent), map = twDist2) + expand_limits(x = twDist2$long, y = twDist2$lat)
distanceCenter <- ddply(twDist2, .(id), summarize, latCenter = mean(lat), longCenter = mean(long))
ggplot() + geom_map(data=twIndex, aes(map_id = id, fill = oldPercent), map = twDist2) + expand_limits(x = twDist2$long, y = twDist2$lat) + scale_fill_gradient2(low = "white",  mid = "palevioletred1", midpoint = mean(twIndex$oldPercent), high = muted("palevioletred4"), limits = c(min(twIndex$oldPercent)-3, max(twIndex$oldPercent)+3))+geom_text(data = distanceCenter, aes(x = longCenter, y = latCenter, label = id, size = 0.2))+xlab("")+ylab("")+ggtitle("Older Population in Taiwan")
